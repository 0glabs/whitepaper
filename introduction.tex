\section{Introduction}

Artificial Intelligence (AI) has rapidly evolved into a powerful tool that is transforming industries and reshaping our everyday lives. 
However, the journey of AI development and deployment is often confined within centralized platforms, leading to significant concerns around privacy, fairness, and alignment. 
As we continue to witness the proliferation of AI, a critical trend is emerging: the democratization of AI through decentralization.

Centralized AI platforms, while convenient and powerful, introduce several critical issues. When a handful of entities control the AI infrastructure, they also control the data and the algorithms. 
This centralization creates a scenario where users' privacy can be compromised, either through data breaches or through misuse of personal information.
Moreover, the decision-making processes of AI systems can become opaque, making it difficult to ensure fairness and accountability. 
Alignment, or ensuring that AI systems act in accordance with human values and intentions, becomes particularly challenging in centralized environments. 
If AI development is monopolized by a few organizations, the risk of misalignment with the broader population’s values increases, potentially leading to outcomes that are beneficial to some but detrimental to others.

Decentralization offers a promising alternative. 
By moving AI workflows into a decentralized environment, the entire process becomes more transparent. 
Every step of the AI lifecycle—from data collection and processing to model training and deployment—can be tracked, audited, and verified. 
This transparency helps in addressing concerns about provenance (where the data comes from) and attribution (who owns or has contributed to the data and models). 
Furthermore, a decentralized approach makes it easier to ensure that AI systems are aligned with a broader and more diverse set of values, as it is less likely to be controlled by a single entity with a narrow agenda.

However, while the benefits of decentralizing AI are clear, the challenges are equally daunting. 
The AI workflow is inherently complex, involving massive amounts of data processing and neural network computing. 
Replicating this workflow in a decentralized environment is non-trivial. 
It requires well-designed abstraction and super scalable infrastructure that can handle the demands of AI workloads while maintaining the benefits of decentralization.

\project makes a first step towards the ultimate solution by providing a decentralized AI operating system through a modular and layered architecture design. 
It consists of a storage network, a data availability (DA) network, and a data serving network, managing the computing and storage resources in all these networks in a permissionless way, and orchestrating them organically with the separate \project consensus network.
All of these network components utilize carefully designed and innovative sharding mechanisms to achieve infinite horizontal scalability, thereby removing obstacles to the true democratization of AI.

The \project storage network consists of a bunch of storage nodes connected through a peer-to-peer gossip network. 
Each data block written into the storage network accompanies a transaction on the consensus network to record the commitment and the ordering of the data. 
To scale the data throughput infinitely, the storage network is organized in a partitioned way and connects to an arbitrary number of consensus networks that run in parallel and independently. 
The data requests from different independent applications may be written into different storage partitions and their data commitments can be recorded into different consensus networks simultaneously.
All the consensus networks share the same set of validators with the same staking status so that they keep the same level of security.    
Each storage node actively participates in a mining process by submitting proof of accessibility for a specific piece of data to a smart contract deployed on a consensus network. 
Once the proof is validated by the smart contract, the storage node gets rewarded accordingly. 
This incentive-based mechanism rewards the nodes for contributions rather than punishing them for misbehaviors, so it can better encourage nodes to participate in the maintenance of the network, and hence can promote the network to achieve better scalability in practice.
\projabbrev storage is also designed as a general storage system with multiple stacks of abstractions and structures including an append-only log layer for archiving unstructured data and a key-value layer for managing mutable and structured data. 
This allows \projabbrev to support reliable data indexing and a greater variety of data types for AI processing.  

The \project DA network comes from the demand for off-chain verification of executed states in Layer2s or decentralized AI networks with optimistic mechanism. 
Specifically, the off-chain verifier, usually the light client, needs to be able to access the entire transaction history data to verify the execution of the transactions.
In Layer2 scenarios, the blocks containing executed transactions in Layer 2 networks need to be published and stored somewhere for light client to conduct further verification.
Similar requirements also exist in decentralized AI infrastructures where the results of training or inference tasks on devices in the networks need to be further verified due to the demands of users or system incentives. 
For example, in ORA/OpML~\cite{opml} scenarios, it requires participants to provide fraud proofs for specific AI tasks during a challenge window and the proofs may need to contain the data and models used in those tasks. 
Some incentive mechanisms may also require randomly choosing and verifying old historical tasks to give rewards accordingly, and hence to achieve a good trade-off between verification cost and effectiveness. 

In contrast to other alternative DA solutions, e.g., Celestia~\cite{celestia} and EigenDA~\cite{eigenda}, \projabbrev DA targets to the ultimate scalability and security.
It embraces the idea of separating the workflow of data availability into both the Data Publishing Lane and the Data Storage Lane. 
The large volume of data transfers happen on the Data Storage Lane that is supported by the DA nodes with horizontal scalability through erasure-coding based data slicing, while the Data Publishing Lane guarantees the data availability property by checking the aggregated signatures of the corresponding DA nodes on the consensus network, which only requires tiny data flowing through the consensus protocol to avoid the broadcasting bottleneck. 

The DA nodes in the network also need to be stakers on \projabbrev consensus and form a series of quorums with majority honesty assumption. 
This is the security foundation of the data availability.
The quorums of \projabbrev are constructed randomly by the consensus system through verifiable random function (VRF) which theoretically guarantees the same distribution of the honest participants as in the validator set of the entire consensus network, so that the data availability client cannot collude with them.
In other words, as subsets of all the validators, the quorums share the same security property as the entire consensus network.
The aggregated signatures of the quorum will be submitted to the consensus of \projabbrev for data availability confirmation, which can be orders of magnitude faster and more efficient than Ethereum. 
Combined with the multi-consensus design of \projabbrev with infinite scalability, the consensus protocol will not become the bottleneck of the data throughput.
In addition, as with EigenDA, the data availability client needs to conduct erasure coding for the data to have it split into chunks. In practice, this process is very costly and can be the major bottleneck for the throughput of a single client. 
\projabbrev provides a GPU-accelerated solution for the erasure coding process to significantly speed it up.

Further, in order to support efficient and scalable AI computation and data access, \projabbrev proposes a decentralized serving network.
It is a general framework that can support all kinds of serving workloads including data retrieval, AI inference, and even AI training tasks. 
It consists of a set of smart contracts and distributed software to allow any type of services to integrate into the decentralized network.
Attached with \projabbrev serving framework SDK, a service provider can register and publish the service type, pricing, and verification method into the smart contract.
A user can discover a service that she is willing to use through our platform and pre-pay certain amount of \projabbrev tokens to the smart contract.
She then can directly interact with the service, sending request, receiving and verifying the response, and acknowledging the response with signature.
The service provider maintains the request/response sequence with user acknowledgment and send it to the smart contract for settlement at any time to get the corresponding reward from the user paid fees. 
Through this way, the service workload can be perfectly partitioned for horizontal scalability and the blockchain overhead is minimized. 
Since all the service process traces are sent to the smart contract for settlement, these information can also be used to evaluate the contribution and quality of the services in the entire network, which enables the appropriate and fair incentives for better service providers.





%The trade-off between scalability and security in blockchain systems has given rise to a demand for off-chain verification of executed states. This further introduces the data availability problem~\cite{da}. Specifically, the off-chain verifier, usually the light client, needs to be able to access the entire transaction history data to verify the execution of the transactions. 
%Currently, a major application scenario for data availability is the Layer 2 networks of Ethereum~\cite{Ethereum}. The blocks containing executed transactions in Layer 2 networks need to be published and stored somewhere for light client to conduct further verification. Similar requirements also exist in decentralized AI infrastructures where the results of training or inference tasks on devices in decentralized networks need to be further verified due to the demands of users or system incentives. 
%For example, in ORA/OpML~\cite{opml} scenario where an optimistic mechanism is employed, it requires participants to provide fraud proofs for specific AI tasks during a challenge window and the proofs may need to contain the data and models used in those tasks. 
%Some incentive mechanisms may also require randomly choosing and verifying old historical tasks to give rewards accordingly, and hence to achieve a good trade-off between verification cost and effectiveness. 

%Public Layer 1 blockchain like Ethereum can be an option to provide the service for data availability usage, but it brings significant, sometimes unacceptable, extra costs to Layer 2 network applications (e.g., \$140 for one OP~\cite{op} block with ~218KB). It is also extremely inefficient and unscalable with a data throughput at about 83 KBps and a finality time of 12 minutes. Recently, Celestia~\cite{celestia} was proposed to provide a cheaper solution by separating the data availability module from Ethereum to become a shared platform. It is itself a consensus-based distributed ledger that is specifically used to serve data availability requests. Although it is cheaper due to the lower gas price for storing transaction data, it is still far from a scalable solution because every data block of the data availability request has to be broadcast in its consensus network to get the multi-signed certification from all the validators. This prevents it from effectively utilizing aggregated network bandwidth, resulting in an overall data throughput unable to surpass the bottleneck of 10 MBps, which makes it hard to support the huge amount of data availability requests brought by the proliferation of Layer 2 or Layer 3 networks, and decentralized AI platforms. 
%Furthermore, Celestia does not have a scalable storage solution for data blobs with availability requirements, meaning it must rely on a massive data availability sampling done by light clients. This further slows down the system due to the enormous bandwidth consumption and forces its block time to more than 10 seconds. 

%In contrast, EigenDA~\cite{eigenda} provides a data availability solution with horizontal scalability in design. Instead of employing a consensus layer, it relies on a quorum of data availability nodes that hold a security assumption where the majority of nodes are honest. Each data availability node is an EigenLayer~\cite{eigenlayer} restaker that stakes its ETH to participate in the quorum protocol. The protocol does not involve a well-designed incentive mechanism for storing data blocks. Instead, it introduces a slashing-based mechanism to punish the node that breaks its promise on the data storage and access service. However, this brings to the node the risk of unexpected loss of the staked assets since it is hard to distinguish between malicious data withholding and an accidental failure (probably caused by a DDoS attack). This greatly reduces the willingness of nodes to participate which limits the actual scalability that it can achieve. 
%In addition, although the data blocks can be maintained in the nodes with availability, without a general decentralized storage system, it is hard to maintain the availability of data index, and hence cannot achieve the available retrieval.

%One important factor in a quorum-based system is the quorum construction. In EigenDA, the process of constructing the quorums is too arbitrary. The data availability node operator itself decides which quorum to join and the clients choose by themselves which quorum to use for data availability service. In this case, it is easy for malicious clients to collude with data availability nodes to fool users, jeopardizing security.
%Furthermore, the aggregated signatures of the data availability nodes in a quorum need to be submitted to the Ethereum smart contract for further verification to confirm the data availability property, which can also become a significant bottleneck for large data throughput.  

%To better address the scalability issue of data availability, we propose \project, the first data availability system with a built-in general-purpose storage layer that is infinitely scalable and decentralized. The scalability of \projabbrev comes from the following architecture design points. 
%First, the basic setup of the storage layer of \projabbrev consists of a storage network connecting with a separate consensus network. Each data block written into the storage network accompanies a transaction on the consensus network to record the commitment and the ordering of the data. 
%To scale the data throughput infinitely, the storage network is organized in a partitioned way and connects to an arbitrary number of consensus networks that run in parallel and independently. 
%The data requests from different independent applications may be written into different storage partitions and their data commitments can be recorded into different consensus networks simultaneously.
%All the consensus networks share the same set of validators with the same staking status so that they keep the same level of security.    
%Each storage node actively participates in a mining process by submitting proof of accessibility for a specific piece of data to a smart contract deployed on a consensus network. Once the proof is validated by the smart contract, the storage node gets rewarded accordingly. 
%This incentive-based mechanism rewards the nodes for contributions rather than punishing them for misbehaviors, so it can better encourage nodes to participate in the maintenance of the network, and hence can promote the network to achieve better scalability in practice.

%Secondly, \projabbrev embraces the idea of separating the workflow of data availability into both the Data Publishing Lane and the Data Storage Lane. 
%The large volume of data transfers happens on the Data Storage Lane that is supported by the storage layer which achieves horizontal scalability through well-designed partitioning, while the Data Publishing Lane guarantees the data availability property by checking the aggregated signatures of the corresponding storage nodes on the consensus network, which only requires tiny data flowing through the consensus protocol to avoid the broadcasting bottleneck. 
%Note that, recently, there have been debates within the Ethereum community regarding data availability, tending to narrowly define it as data publication. However, this can be misleading. In fact, data storage is an integral part of data availability because it must answer the question of where the data is published. Without a well-designed and reliable storage layer, systems like Celestia have to inject entire data blocks into the consensus system and store them on full nodes.

%The storage nodes are important for \projabbrev both the data availability service and data storage, with those partaking in data availability being required to participate as consensus network validators. Similar to EigenDA, the data availability of \projabbrev is based on a majority honesty assumption in the quorum of storage nodes with staking. Differently, the data availability quorums of \projabbrev are constructed randomly by the consensus system through verifiable random function (VRF) which theoretically guarantees the same distribution of the honest participants as in the validator set of the entire consensus network, so that the data availability client cannot collude with them.
%In other words, as subsets of all the validators, the quorums share the same security property as the entire consensus network.
%The aggregated signatures of the quorum will be submitted to the consensus of \projabbrev for data availability confirmation, which can be orders of magnitude faster and more efficient than Ethereum. 
%Combined with the multi-consensus design of \projabbrev with infinite scalability, the consensus protocol will not become the bottleneck of the data throughput.
%In addition, as with EigenDA, the data availability client needs to conduct erasure coding for the data to have it split into chunks. In practice, this process is very costly and can be the major bottleneck for the throughput of a single client. \projabbrev provides a GPU-accelerated solution for the erasure coding process to significantly speed it up.   
 

%Last but not least, \projabbrev storage is also designed as a general storage system with multiple stacks of abstractions and structures including an append-only log layer for archiving unstructured data and a key-value layer for managing mutable and structured data. This allows \projabbrev to support reliable data indexing and a greater variety of data availability scenarios.  




 
%In the traditional cloud and big-data industry, the storage infrastructure plays a vital role on supporting efficient and scalable data access and maintenance. To handle the data at Internet scale, the storage systems are designed and developed in a distributed way in large data center usually across thousands of machines and disks. Although embracing the distributed technologies, the entire data centers are still controlled and managed by the giant Internet companies like Google, Meta, Microsoft, Alibaba, etc., and hence it requires users to trust the integrity of those monopolists on soundly maintaining and manipulating their data. 

%The recent trend of Web3.0 has brought a new wave of decentralizing the data storage infrastructures to make them trustless. In such systems, the data can permanently exist and can really belong to their personal owners rather than owned by the monopolist. The privacy and the value of the data can be better protected. Due to the transparency of the data usage history and the combination with the distributed ledger of blockchain, the users’ data can be smoothly involved into the economic ecosystems of the blockchain world, and hence it enables the users to fairly enjoy the benefit associated with the value of their data.

%In the design and development process of the large-scale distributed systems and applications, the modularity and the abstraction are the two of the most critical consideration factors. The good modularity and abstraction can maximally avoid redundant engineering efforts and boost productivity through abstracting away the complexity of some components from the developers who work on others. This in turn may largely scale the development of the ecosystem of the underlying infrastructures and platforms, and can also make it easy for the systems to achieve scalable performance due to the composability it provides. A well-designed abstraction for the large-scale systems may often lead to a huge leap of progress in the industry. There are many cases of this. One example is MapReduce~\cite{mapreduce} which abstracts away the details of parallelization, fault-tolerance, locality optimization, and load balancing in the cluster from the developers of data processing applications. After Google proposed MapReduce, many Internet and software giant companies follow this programming paradigm to significantly improve the productivity of their data centers. Another example is TensorFlow~\cite{tensorflow} which employs the data-flow graph abstraction for the deep neural network computation and abstracts away the complex distributed GPU optimizations from the developers of deep learning model. TensorFlow ushers the AI industry into the era of large neural network model through enabling deep learning computation over the large-scale GPU cluster. 

%In the storage area, key-value abstraction is the most influential one since it defines the most general indexing schemes for data access. Many large-scale distributed storage systems provide or use variants of key-value interfaces. One example is Google’s Bigtable~\cite{bigtable} which manages peta-scale structured data across thousands of commodity servers. Many projects in Google store data in Bigtable including web indexing, Google Analytics, and Google Finance, etc. Later on, the system evolved to be Megastore~\cite{megastore} and Spanner~\cite{spanner} which integrate the relational and transactional semantics at globally-distributed scale and support hundreds of applications. The similar scenario is in PingCAP’s TiKV and TiDB which is a popular open-sourced distributed storage system with transactional key-value abstraction and relational database on top of it. There are also many other DHT (distributed hash table) based systems, e.g., Meta’s Cassandra~\cite{cassandra}, Amazon’s DynamoDB~\cite{dynamodb}, etc., which provide the key-value interface for their applications. 

%It is clear that to migrate more traditional Internet applications to Web3.0 world, we also need to provide the decentralized storage system with well-designed abstraction and modularity. Although Filecoin and Arweave have enabled a wave of decentralizing the storage system, they are simply the archive layer with the simplest data upload API which is clearly not enough for full-fledged applications.

%\project is to redesign the decentralized full storage stack in Web3.0 with the well-designed abstractions and modularity. Besides the basic Web3.0 features, the abstraction and modular design needs to facilitate the system to achieve the scalability from both the data access performance and the pace of system development. When thinking about this effort, one naïve question is why not simply scaling the layer-1 blockchain, e.g., through sharding. Because the smart contract in blockchain usually provides key-value semantics with mapping support, it is a natural thought that scaling the blockchain through sharding would result in a scalable decentralized key-value store. However, scaling the layer-1 blockchain system with sharding is an astounding job which has huge challenges because it mixes the complexity of consensus of distributed ledger and the sharding protocol. In this mixture, it is very difficult to find a clean boundary between these two features, and hence makes the whole system hard to be implemented correctly. Look at how many years the sharding upgrade of Ethereum 2.0 takes from its proposal to rolling out. 

%The design of \project is inspired by a series of research work Corfu~\cite{corfu}, Tango~\cite{tango}, and Delos~\cite{delos} which are also production systems deployed in giant companies like Microsoft and Meta. We observe that the full stack of distributed storage systems often employs a layered design. The lowest layer is an archive system which provides permanent persistency and append-only property. This layer can be designed with a log abstraction. To decentralize the log layer, we identify two pieces of logic in the log system that can be decoupled and decentralized separately. One is the data maintenance and the other is the data ordering. The data maintenance can be performed with a network of storage nodes through Proof-of-Storage while the data ordering can be simply fulfilled through employing the smart contract of one existing layer-1 blockchain. This modular design enables the decentralized log system to seamlessly leverage the evolution of layer-1 blockchain technology for better throughput and latency. In addition, through grafting to multiple layer-1 blockchains, the log system can easily scale linearly in a sharded way. And furthermore, through choosing to integrate with layer-1 blockchain that is EVM-compatible and has prosperous ecosystem, users can directly interact with the system through Metamask wallet which has much better user experience and familiarity, and the system token can easily join the token family in the tokenomics of the blockchain.
%On top of the decentralized log, \project introduces the key-based object structure into the log entry, which enables the serving nodes to consistently construct the state of an upper layer of key-value store through playing the entries in the recorded sequence in the log. This key-value layer enables the mutability of the data and allows applications like decentralized file system or social network like Twitter. Upon the key-value layer, \project further builds transactional semantics to provide ACID for applications with concurrent data update, e.g., Google doc with collaborative editing. The total order provided by the underlying log layer further makes the concurrency control of the transactional processing easy to be implemented. 
