\section{Introduction}

The trade-off between scalability and security in blockchain systems has given rise to a demand for off-chain verification of executed states. This further introduces the data availability problem~\cite{da}. Specifically, the off-chain verifier, usually the light client, needs to be able to access the entire transaction history data to verify the execution of the transactions. 
Currently, a major application scenario for data availability is the Layer 2 networks of Ethereum~\cite{Ethereum}. The blocks containing executed transactions in Layer 2 networks need to be published and stored somewhere for light client to conduct further verification. The similar requirements also exist in decentralized AI infrastructures where the results of training or inference tasks on devices in decentralized networks need to be further verified due to the demands of users or system incentives. 
For example, in ORA/OpML~\cite{opml} scenario where optimistic mechanism is employed, it requires participants to provide fraud proofs for specific AI tasks during a challenge window and the proofs may need to contain the data and models used in those tasks. 
And some incentive mechanisms may also require randomly choosing old historical tasks to verify to give reward accordingly, and hence to achieve a good trade-off between verification cost and effectiveness. 

Public Layer 1 blockchain like Ethereum can be an option to provide the service for data availability usage, but it brings significant, sometimes unacceptable, extra costs to Layer 2 applications (e.g., \$140 for one OP~\cite{op} block with ~218KB). It is also extremely inefficient and unscalable with a data throughput at about 83 KBps and a finality time of 12 minutes. Recently, Celestia~\cite{celestia} was proposed to provide a cheaper solution by separating the data availability module from Ethereum to become a shared platform. It is itself a consensus-based distributed ledger that is specifically used to serve data availability requests. Although it is cheaper due to the lower gas price for storing transaction data, it is still far from a scalable solution because every data block of the data availability request has to be broadcast in its consensus network to get the multi-signed certification from all the validators. This prevents it from effectively utilizing aggregated network bandwidth, resulting in an overall data throughput unable to surpass the bottleneck of 10 MBps, which makes it hard to support the huge amount of data availability requests brought by the proliferation of the Layer 2 and Layer 3 networks and the decentralized AI platforms.

EigenDA~\cite{eigenda} further provides a data availability solution with horizontal scalability in design. Instead of employing a consensus layer, it relies on a quorum of data availability nodes that hold a security assumption where majority nodes are honesty. Each data availability node is an EigenLayer~\cite{eigenlayer} restaker who stakes its ETH to participate the quorum protocol. The protocol does not involve a well designed incentive mechanism for storing data blocks. Instead, it introduces a slashing-based mechanism to punish the node that breaks its promise on the data storage and access service. However, this brings to the node the risk of unexpected loss of the staked assets since it is hard to distinguish between the malicious data withholding and an accidental failure (probably caused by a DDOS attack). This would greatly reduces the willingness of nodes to participate, and hence limit the actual scalability that it can achieve. 
In addition, although the data blocks can be maintained in the nodes with availability, without a general decentralized storage system, it is hard to maintain the availability of data index, and hence cannot achieve the available retrieval.

To better address the scalability issue of data availability, we propose \project (\projabbrev in short), the first data availability system with a built-in general purpose storage layer that is infinitely scalable and decentralized. The scalability of \projabbrev comes from the following architecture design points. 
First, the basic setup of the storage layer of \projabbrev consists of a storage network connecting with a separate consensus network. Each data block written into the storage network accompanies a transaction on the consensus network to record the commitment and the ordering of the data. 
To scale the data throughput infinitely, the storage network is organized in a partitioned way and connects to arbitrary number of consensus networks that run in parallel and independently. 
The data requests from different independent applications may be written into different storage partitions and their data commitments can be recorded into different consensus networks simultaneously.
All the consensus networks share the same set of validators with the same staking status so that they keep the same level of security.    
Each storage node actively participates a mining process by submitting the proof of accessibility for specific piece of data to a smart contract deployed on a consensus network. Once the proof is validated by the smart contract, the storage node gets rewarded accordingly. 
The partitioning is enabled through rewarding more to the node for storing the specific data that belong to the same partition with the node. This incentive-based mechanism rewards the nodes for contributions rather than punishing them for misbehaviors, so it can better encourage nodes to participate in the maintenance of the network, and hence can promote the network to achieve better scalability in practice.

Secondly, \projabbrev embraces the idea of separating the workflow of data availability into data publishing lane and data storage lane. 
Large volume of data transfers happen on the data storage lane that is supported by the storage layer which achieves the horizontal scalability through well designed partitioning, while the data publishing lane guarantees the data availability property through consensus of data availability sampling which only requires tiny data flowing through the consensus protocol to avoid the broadcasting bottleneck. 
The consensus result of data availability sampling can be further bridged out to the external L1 blockchains, which may have higher level of decentralization and security, e.g., Ethereum, for attestation. 
Note that, recently, there have been debates within the Ethereum community regarding data availability, tending to narrowly define it as data publication. However, this can be misleading. In fact, data storage is an integral part of data availability because it must answer the question of where the data is published. Therefore, without a well-designed and reliable storage system, Celestia has to inject entire data blocks into the consensus system and store them on full nodes.

\projabbrev storage is also designed as a general storage system with multiple stacks of abstractions and structures including an append-only log layer for archiving unstructured data and also a key-value layer for managing mutable and structured data. This allows \projabbrev to support reliable data indexing and a greater variety of availability data types from Layer 2 and AI scenarios.  




 
%In the traditional cloud and big-data industry, the storage infrastructure plays a vital role on supporting efficient and scalable data access and maintenance. To handle the data at Internet scale, the storage systems are designed and developed in a distributed way in large data center usually across thousands of machines and disks. Although embracing the distributed technologies, the entire data centers are still controlled and managed by the giant Internet companies like Google, Meta, Microsoft, Alibaba, etc., and hence it requires users to trust the integrity of those monopolists on soundly maintaining and manipulating their data. 

%The recent trend of Web3.0 has brought a new wave of decentralizing the data storage infrastructures to make them trustless. In such systems, the data can permanently exist and can really belong to their personal owners rather than owned by the monopolist. The privacy and the value of the data can be better protected. Due to the transparency of the data usage history and the combination with the distributed ledger of blockchain, the usersâ€™ data can be smoothly involved into the economic ecosystems of the blockchain world, and hence it enables the users to fairly enjoy the benefit associated with the value of their data.

%In the design and development process of the large-scale distributed systems and applications, the modularity and the abstraction are the two of the most critical consideration factors. The good modularity and abstraction can maximally avoid redundant engineering efforts and boost productivity through abstracting away the complexity of some components from the developers who work on others. This in turn may largely scale the development of the ecosystem of the underlying infrastructures and platforms, and can also make it easy for the systems to achieve scalable performance due to the composability it provides. A well-designed abstraction for the large-scale systems may often lead to a huge leap of progress in the industry. There are many cases of this. One example is MapReduce~\cite{mapreduce} which abstracts away the details of parallelization, fault-tolerance, locality optimization, and load balancing in the cluster from the developers of data processing applications. After Google proposed MapReduce, many Internet and software giant companies follow this programming paradigm to significantly improve the productivity of their data centers. Another example is TensorFlow~\cite{tensorflow} which employs the data-flow graph abstraction for the deep neural network computation and abstracts away the complex distributed GPU optimizations from the developers of deep learning model. TensorFlow ushers the AI industry into the era of large neural network model through enabling deep learning computation over the large-scale GPU cluster. 

%In the storage area, key-value abstraction is the most influential one since it defines the most general indexing schemes for data access. Many large-scale distributed storage systems provide or use variants of key-value interfaces. One example is Googleâ€™s Bigtable~\cite{bigtable} which manages peta-scale structured data across thousands of commodity servers. Many projects in Google store data in Bigtable including web indexing, Google Analytics, and Google Finance, etc. Later on, the system evolved to be Megastore~\cite{megastore} and Spanner~\cite{spanner} which integrate the relational and transactional semantics at globally-distributed scale and support hundreds of applications. The similar scenario is in PingCAPâ€™s TiKV and TiDB which is a popular open-sourced distributed storage system with transactional key-value abstraction and relational database on top of it. There are also many other DHT (distributed hash table) based systems, e.g., Metaâ€™s Cassandra~\cite{cassandra}, Amazonâ€™s DynamoDB~\cite{dynamodb}, etc., which provide the key-value interface for their applications. 

%It is clear that to migrate more traditional Internet applications to Web3.0 world, we also need to provide the decentralized storage system with well-designed abstraction and modularity. Although Filecoin and Arweave have enabled a wave of decentralizing the storage system, they are simply the archive layer with the simplest data upload API which is clearly not enough for full-fledged applications.

%\project is to redesign the decentralized full storage stack in Web3.0 with the well-designed abstractions and modularity. Besides the basic Web3.0 features, the abstraction and modular design needs to facilitate the system to achieve the scalability from both the data access performance and the pace of system development. When thinking about this effort, one naÃ¯ve question is why not simply scaling the layer-1 blockchain, e.g., through sharding. Because the smart contract in blockchain usually provides key-value semantics with mapping support, it is a natural thought that scaling the blockchain through sharding would result in a scalable decentralized key-value store. However, scaling the layer-1 blockchain system with sharding is an astounding job which has huge challenges because it mixes the complexity of consensus of distributed ledger and the sharding protocol. In this mixture, it is very difficult to find a clean boundary between these two features, and hence makes the whole system hard to be implemented correctly. Look at how many years the sharding upgrade of Ethereum 2.0 takes from its proposal to rolling out. 

%The design of \project is inspired by a series of research work Corfu~\cite{corfu}, Tango~\cite{tango}, and Delos~\cite{delos} which are also production systems deployed in giant companies like Microsoft and Meta. We observe that the full stack of distributed storage systems often employs a layered design. The lowest layer is an archive system which provides permanent persistency and append-only property. This layer can be designed with a log abstraction. To decentralize the log layer, we identify two pieces of logic in the log system that can be decoupled and decentralized separately. One is the data maintenance and the other is the data ordering. The data maintenance can be performed with a network of storage nodes through Proof-of-Storage while the data ordering can be simply fulfilled through employing the smart contract of one existing layer-1 blockchain. This modular design enables the decentralized log system to seamlessly leverage the evolution of layer-1 blockchain technology for better throughput and latency. In addition, through grafting to multiple layer-1 blockchains, the log system can easily scale linearly in a sharded way. And furthermore, through choosing to integrate with layer-1 blockchain that is EVM-compatible and has prosperous ecosystem, users can directly interact with the system through Metamask wallet which has much better user experience and familiarity, and the system token can easily join the token family in the tokenomics of the blockchain.
%On top of the decentralized log, \project introduces the key-based object structure into the log entry, which enables the serving nodes to consistently construct the state of an upper layer of key-value store through playing the entries in the recorded sequence in the log. This key-value layer enables the mutability of the data and allows applications like decentralized file system or social network like Twitter. Upon the key-value layer, \project further builds transactional semantics to provide ACID for applications with concurrent data update, e.g., Google doc with collaborative editing. The total order provided by the underlying log layer further makes the concurrency control of the transactional processing easy to be implemented. 
