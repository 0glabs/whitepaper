\section{Introduction}

In the traditional cloud and big-data industry, the storage infrastructure plays a vital role on supporting efficient and scalable data access and maintenance. To handle the data at Internet scale, the storage systems are designed and developed in a distributed way in large data center usually across thousands of machines and disks. Although embracing the distributed technologies, the entire data centers are still controlled and managed by the giant Internet companies like Google, Meta, Microsoft, Alibaba, etc., and hence it requires users to trust the integrity of those monopolists on soundly maintaining and manipulating their data. 

The recent trend of Web3.0 has brought a new wave of decentralizing the data storage infrastructures to make them trustless. In such systems, the data can permanently exist and can really belong to their personal owners rather than owned by the monopolist. The privacy and the value of the data can be better protected. Due to the transparency of the data usage history and the combination with the distributed ledger of blockchain, the users’ data can be smoothly involved into the economic ecosystems of the blockchain world, and hence it enables the users to fairly enjoy the benefit associated with the value of their data.

In the design and development process of the large-scale distributed systems and applications, the modularity and the abstraction are the two of the most critical consideration factors. The good modularity and abstraction can maximally avoid redundant engineering efforts and boost productivity through abstracting away the complexity of some components from the developers who work on others. This in turn may largely scale the development of the ecosystem of the underlying infrastructures and platforms, and can also make it easy for the systems to achieve scalable performance due to the composability it provides. A well-designed abstraction for the large-scale systems may often lead to a huge leap of progress in the industry. There are many cases of this. One example is MapReduce~\cite{mapreduce} which abstracts away the details of parallelization, fault-tolerance, locality optimization, and load balancing in the cluster from the developers of data processing applications. After Google proposed MapReduce, many Internet and software giant companies follow this programming paradigm to significantly improve the productivity of their data centers. Another example is TensorFlow~\cite{tensorflow} which employs the data-flow graph abstraction for the deep neural network computation and abstracts away the complex distributed GPU optimizations from the developers of deep learning model. TensorFlow ushers the AI industry into the era of large neural network model through enabling deep learning computation over the large-scale GPU cluster. 

In the storage area, key-value abstraction is the most influential one since it defines the most general indexing schemes for data access. Many large-scale distributed storage systems provide or use variants of key-value interfaces. One example is Google’s Bigtable~\cite{bigtable} which manages peta-scale structured data across thousands of commodity servers. Many projects in Google store data in Bigtable including web indexing, Google Analytics, and Google Finance, etc. Later on, the system evolved to be Megastore~\cite{megastore} and Spanner~\cite{spanner} which integrate the relational and transactional semantics at globally-distributed scale and support hundreds of applications. The similar scenario is in PingCAP’s TiKV and TiDB which is a popular open-sourced distributed storage system with transactional key-value abstraction and relational database on top of it. There are also many other DHT (distributed hash table) based systems, e.g., Meta’s Cassandra~\cite{cassandra}, Amazon’s DynamoDB~\cite{dynamodb}, etc., which provide the key-value interface for their applications. 

It is clear that to migrate more traditional Internet applications to Web3.0 world, we also need to provide the decentralized storage system with well-designed abstraction and modularity. Although Filecoin and Arweave have enabled a wave of decentralizing the storage system, they are simply the archive layer with the simplest data upload API which is clearly not enough for full-fledged applications.

\project is to redesign the decentralized full storage stack in Web3.0 with the well-designed abstractions and modularity. Besides the basic Web3.0 features, the abstraction and modular design needs to facilitate the system to achieve the scalability from both the data access performance and the pace of system development. When thinking about this effort, one naïve question is why not simply scaling the layer-1 blockchain, e.g., through sharding. Because the smart contract in blockchain usually provides key-value semantics with mapping support, it is a natural thought that scaling the blockchain through sharding would result in a scalable decentralized key-value store. However, scaling the layer-1 blockchain system with sharding is an astounding job which has huge challenges because it mixes the complexity of consensus of distributed ledger and the sharding protocol. In this mixture, it is very difficult to find a clean boundary between these two features, and hence makes the whole system hard to be implemented correctly. Look at how many years the sharding upgrade of Ethereum 2.0 takes from its proposal to rolling out. 

The design of \project is inspired by a series of research work Corfu~\cite{corfu}, Tango~\cite{tango}, and Delos~\cite{delos} which are also production systems deployed in giant companies like Microsoft and Meta. We observe that the full stack of distributed storage systems often employs a layered design. The lowest layer is an archive system which provides permanent persistency and append-only property. This layer can be designed with a log abstraction. To decentralize the log layer, we identify two pieces of logic in the log system that can be decoupled and decentralized separately. One is the data maintenance and the other is the data ordering. The data maintenance can be performed with a network of storage nodes through Proof-of-Storage while the data ordering can be simply fulfilled through employing the smart contract of one existing layer-1 blockchain. This modular design enables the decentralized log system to seamlessly leverage the evolution of layer-1 blockchain technology for better throughput and latency. In addition, through grafting to multiple layer-1 blockchains, the log system can easily scale linearly in a sharded way. And furthermore, through choosing to integrate with layer-1 blockchain that is EVM-compatible and has prosperous ecosystem, users can directly interact with the system through Metamask wallet which has much better user experience and familiarity, and the system token can easily join the token family in the tokenomics of the blockchain.
On top of the decentralized log, \project introduces the key-based object structure into the log entry, which enables the serving nodes to consistently construct the state of an upper layer of key-value store through playing the entries in the recorded sequence in the log. This key-value layer enables the mutability of the data and allows applications like decentralized file system or social network like Twitter. Upon the key-value layer, \project further builds transactional semantics to provide ACID for applications with concurrent data update, e.g., Google doc with collaborative editing. The total order provided by the underlying log layer further makes the concurrency control of the transactional processing easy to be implemented. 
